diff --git a/iq_learn/.gitignore b/iq_learn/.gitignore
deleted file mode 100644
index 2021cd6..0000000
--- a/iq_learn/.gitignore
+++ /dev/null
@@ -1,129 +0,0 @@
-# Byte-compiled / optimized / DLL files
-__pycache__/
-*.py[cod]
-*$py.class
-
-# C extensions
-*.so
-
-# Distribution / packaging
-.Python
-build/
-develop-eggs/
-dist/
-downloads/
-eggs/
-.eggs/
-lib/
-lib64/
-parts/
-sdist/
-var/
-wheels/
-pip-wheel-metadata/
-share/python-wheels/
-*.egg-info/
-.installed.cfg
-*.egg
-MANIFEST
-
-# PyInstaller
-#  Usually these files are written by a python script from a template
-#  before PyInstaller builds the exe, so as to inject date/other infos into it.
-*.manifest
-*.spec
-
-# Installer logs
-pip-log.txt
-pip-delete-this-directory.txt
-
-# Unit test / coverage reports
-htmlcov/
-.tox/
-.nox/
-.coverage
-.coverage.*
-.cache
-nosetests.xml
-coverage.xml
-*.cover
-*.py,cover
-.hypothesis/
-.pytest_cache/
-
-# Translations
-*.mo
-*.pot
-
-# Django stuff:
-*.log
-local_settings.py
-db.sqlite3
-db.sqlite3-journal
-
-# Flask stuff:
-instance/
-.webassets-cache
-
-# Scrapy stuff:
-.scrapy
-
-# Sphinx documentation
-docs/_build/
-
-# PyBuilder
-target/
-
-# Jupyter Notebook
-.ipynb_checkpoints
-
-# IPython
-profile_default/
-ipython_config.py
-
-# pyenv
-.python-version
-
-# pipenv
-#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
-#   However, in case of collaboration, if having platform-specific dependencies or dependencies
-#   having no cross-platform support, pipenv may install dependencies that don't work, or not
-#   install all needed dependencies.
-#Pipfile.lock
-
-# PEP 582; used by e.g. github.com/David-OConnor/pyflow
-__pypackages__/
-
-# Celery stuff
-celerybeat-schedule
-celerybeat.pid
-
-# SageMath parsed files
-*.sage.py
-
-# Environments
-.env
-.venv
-venv/
-env.bak/
-venv.bak/
-
-# Spyder project settings
-.spyderproject
-.spyproject
-
-# Rope project settings
-.ropeproject
-
-# mkdocs documentation
-/site
-
-# mypy
-.mypy_cache/
-.dmypy.json
-dmypy.json
-
-# Pyre type checker
-.pyre/
-
-.DS_Store
\ No newline at end of file
diff --git a/iq_learn/README.md b/iq_learn/README.md
index b71e536..2ca1883 100644
--- a/iq_learn/README.md
+++ b/iq_learn/README.md
@@ -23,7 +23,7 @@ Our training code is present in `train_iq.py` which implements IQ-Learn on top o
 - To reproduce our Mujoco experiments, see `run_mujoco.sh`
 - To reproduce Atari experiments, see `run_atari.sh`
 - To visualize our recovered state-only rewards on a toy Point Maze environment: 
-    `python -m vis.maze_vis env=pointmaze_right eval.policy=pointmaze agent.init_temperature=1 agent=sac double_q_critic._target_=agent.sac_models.DoubleQCritic`.
+    `python -m vis.maze_vis env=pointmaze_right eval.policy=pointmaze agent.init_temp=1 agent=sac double_q_critic._target_=agent.sac_models.DoubleQCritic`.
     Reward visualizations are saved in `vis/outputs` directory
 
 ## License
diff --git a/iq_learn/agent/sac.py b/iq_learn/agent/sac.py
index c85720a..4e7b55f 100644
--- a/iq_learn/agent/sac.py
+++ b/iq_learn/agent/sac.py
@@ -5,7 +5,7 @@ import torch.nn.functional as F
 from torch.optim import Adam
 import hydra
 
-from utils import soft_update
+from utils.utils import soft_update
 
 
 class SAC(object):
@@ -30,7 +30,7 @@ class SAC(object):
 
         self.actor = hydra.utils.instantiate(agent_cfg.actor_cfg).to(self.device)
 
-        self.log_alpha = torch.tensor(np.log(agent_cfg.init_temperature)).to(self.device)
+        self.log_alpha = torch.tensor(np.log(agent_cfg.init_temp)).to(self.device)
         self.log_alpha.requires_grad = True
         # Target Entropy = âˆ’dim(A) (e.g. , -6 for HalfCheetah-v2) as given in the paper
         self.target_entropy = -action_dim
diff --git a/iq_learn/agent/sac_models.py b/iq_learn/agent/sac_models.py
index 451961a..64878e1 100644
--- a/iq_learn/agent/sac_models.py
+++ b/iq_learn/agent/sac_models.py
@@ -6,7 +6,7 @@ from torch.distributions import Normal
 from torch import distributions as pyd
 from torch.autograd import Variable, grad
 
-import utils
+import utils.utils as utils
 
 LOG_SIG_MAX = 2
 LOG_SIG_MIN = -20
diff --git a/iq_learn/agent/softq.py b/iq_learn/agent/softq.py
index 7bbc009..3d7a96b 100644
--- a/iq_learn/agent/softq.py
+++ b/iq_learn/agent/softq.py
@@ -20,7 +20,7 @@ class SoftQ(object):
         self.critic_tau = agent_cfg.critic_tau
 
         self.critic_target_update_frequency = agent_cfg.critic_target_update_frequency
-        self.log_alpha = torch.tensor(np.log(agent_cfg.init_temperature)).to(self.device)
+        self.log_alpha = torch.tensor(np.log(agent_cfg.init_temp)).to(self.device)
         self.q_net = hydra.utils.instantiate(
             agent_cfg.critic_cfg, args=args, device=self.device).to(self.device)
         self.target_net = hydra.utils.instantiate(agent_cfg.critic_cfg, args=args, device=self.device).to(
@@ -104,6 +104,8 @@ class SoftQ(object):
             y = reward + (1 - done) * self.gamma * next_v
 
         critic_loss = F.mse_loss(self.critic(obs, action), y)
+        logger.log('train_critic/loss', critic_loss, step)
+
         self.critic_optimizer.zero_grad()
         critic_loss.backward()
         self.critic_optimizer.step()
diff --git a/iq_learn/conf/agent/sac.yaml b/iq_learn/conf/agent/sac.yaml
index b446c44..3811fd1 100644
--- a/iq_learn/conf/agent/sac.yaml
+++ b/iq_learn/conf/agent/sac.yaml
@@ -8,7 +8,7 @@ agent:
 
   critic_cfg: ${double_q_critic}
   actor_cfg: ${diag_gaussian_actor}
-  init_temperature: 0.2
+  init_temp: 0.2
 
   alpha_lr: 3e-4
   alpha_betas: [0.9, 0.999]
diff --git a/iq_learn/conf/agent/softq.yaml b/iq_learn/conf/agent/softq.yaml
index 04710c5..14adbb8 100644
--- a/iq_learn/conf/agent/softq.yaml
+++ b/iq_learn/conf/agent/softq.yaml
@@ -10,7 +10,7 @@ agent:
   critic_lr: 1e-4
   critic_betas: [0.9, 0.999]
 
-  init_temperature: 0.01
+  init_temp: 0.01
   critic_target_update_frequency: 4
   critic_tau: 0.1
 
diff --git a/iq_learn/conf/config.yaml b/iq_learn/conf/config.yaml
index da3f782..7e05f50 100644
--- a/iq_learn/conf/config.yaml
+++ b/iq_learn/conf/config.yaml
@@ -16,10 +16,12 @@ train:
   use_target: False
   soft_update: False
 
-eval:
-  policy: 
+expert:
   demos: 1
   subsample_freq: 1
+
+eval:
+  policy: 
   threshold:
   use_baselines: False
   eps: 10
@@ -29,7 +31,6 @@ eval:
 env:
   replay_mem: 50000
   initial_mem: 1280
-  update_steps: 4
   eps_steps: 1000
   eps_window: 100
   learn_steps: 5e5
diff --git a/iq_learn/conf/env/acrobot.yaml b/iq_learn/conf/env/acrobot.yaml
index 56ecd5e..1b5c49f 100644
--- a/iq_learn/conf/env/acrobot.yaml
+++ b/iq_learn/conf/env/acrobot.yaml
@@ -6,10 +6,12 @@ env:
   learn_steps: 2e5
   eval_interval: 1e3
 
-eval:
-  policy: 
+expert:
   demos: 1
   subsample_freq: 5
+
+eval:
+  policy: 
   threshold: 
 
 q_net:
diff --git a/iq_learn/conf/env/ant.yaml b/iq_learn/conf/env/ant.yaml
index 4979bdc..605ea82 100644
--- a/iq_learn/conf/env/ant.yaml
+++ b/iq_learn/conf/env/ant.yaml
@@ -14,12 +14,13 @@ env:
   learn_steps: 5e5
   eval_interval: 5e3
 
-eval:
-  policy: 
+expert:
   demos: 1
   subsample_freq: 1
-  threshold: 4500
 
+eval:
+  policy: 
+  threshold: 4500
 
 agent:
   name: sac
diff --git a/iq_learn/conf/env/cartpole.yaml b/iq_learn/conf/env/cartpole.yaml
index d97b968..bce11e3 100644
--- a/iq_learn/conf/env/cartpole.yaml
+++ b/iq_learn/conf/env/cartpole.yaml
@@ -6,10 +6,12 @@ env:
   learn_steps: 1e5
   eval_interval: 1e3
 
-eval:
-  policy: 
+expert:
   demos: 1
   subsample_freq: 20
+
+eval:
+  policy: 
   threshold: 500
 
 q_net:
diff --git a/iq_learn/conf/env/hopper.yaml b/iq_learn/conf/env/hopper.yaml
index ea8be53..3c5e6c0 100644
--- a/iq_learn/conf/env/hopper.yaml
+++ b/iq_learn/conf/env/hopper.yaml
@@ -14,16 +14,18 @@ env:
   learn_steps: 5e5
   eval_interval: 5e3
 
-eval:
-  policy: 
+expert:
   demos: 1
   subsample_freq: 1
+
+eval:
+  policy: 
   threshold: 3500
 
 
 agent:
   name: sac
-  init_temperature: 1e-2
+  init_temp: 1e-2
 
 log_interval: 500  # Log every this many steps
 num_actor_updates: 1
diff --git a/iq_learn/conf/env/pointmaze_right.yaml b/iq_learn/conf/env/pointmaze_right.yaml
index 295ecd8..40339d7 100644
--- a/iq_learn/conf/env/pointmaze_right.yaml
+++ b/iq_learn/conf/env/pointmaze_right.yaml
@@ -12,11 +12,12 @@ env:
   learn_steps: 2e5
   eval_interval: 5e3
 
+expert:
+  demos: 50
+  subsample_freq: 1
 
 eval:
   policy: 
-  demos: 50
-  subsample_freq: 1
   threshold: -8
   eps: 10
 
diff --git a/iq_learn/conf/env/space.yaml b/iq_learn/conf/env/space.yaml
index 9ed5745..87b7907 100644
--- a/iq_learn/conf/env/space.yaml
+++ b/iq_learn/conf/env/space.yaml
@@ -5,15 +5,16 @@ env:
   demo: SpaceInvadersNoFrameskip-v4_30.pkl
   replay_mem: 150000
   initial_mem: 5000
-  update_steps: 8000
   eps_steps: 1e6
   eps_window: 10
   learn_steps: 10e5
 
-eval:
-  policy: 
+expert:
   demos: 20
   subsample_freq: 1
+
+eval:
+  policy: 
   threshold: 21
   use_baselines: True
   eps: 10
diff --git a/iq_learn/conf/env/walker.yaml b/iq_learn/conf/env/walker.yaml
index 2d6fcf5..42f913d 100644
--- a/iq_learn/conf/env/walker.yaml
+++ b/iq_learn/conf/env/walker.yaml
@@ -14,10 +14,12 @@ env:
   learn_steps: 5e5
   eval_interval: 5e3
 
-eval:
-  policy: 
+expert:
   demos: 1
   subsample_freq: 1
+
+eval:
+  policy: 
   threshold: 5000
 
 
diff --git a/iq_learn/conf/method/iq.yaml b/iq_learn/conf/method/iq.yaml
index 32c8c9b..11b52dd 100644
--- a/iq_learn/conf/method/iq.yaml
+++ b/iq_learn/conf/method/iq.yaml
@@ -2,7 +2,7 @@
 
 method:
   type: iq
-  loss: v0
+  loss: value
 
   constrain: False
   grad_pen: False
@@ -11,7 +11,6 @@ method:
   regularize: False
   div:
 
-
   alpha: 0.5
   lambda_gp: 10
   mix_coeff: 1
diff --git a/iq_learn/envs/point_maze_env.py b/iq_learn/envs/point_maze_env.py
index 7de9a16..f7a19fe 100755
--- a/iq_learn/envs/point_maze_env.py
+++ b/iq_learn/envs/point_maze_env.py
@@ -5,7 +5,7 @@ import mujoco_py
 # from mujoco_py.mjlib import mjlib
 import os
 
-import logger
+import utils.logger as logger
 
 from envs.dynamic_mjc.mjc_models import point_mass_maze
 
diff --git a/iq_learn/expert/expert_dataset.py b/iq_learn/expert/expert_dataset.py
deleted file mode 100644
index 654283a..0000000
--- a/iq_learn/expert/expert_dataset.py
+++ /dev/null
@@ -1,151 +0,0 @@
-from typing import Any, Dict, IO, List, Tuple
-
-import numpy as np
-import pickle
-import torch
-from torch.utils.data import Dataset
-import os
-
-
-class ExpertDataset(Dataset):
-    """Dataset for expert trajectories.
-
-    Assumes expert dataset is a dict with keys {states, actions, rewards, lengths} with values
-    of given shapes below with 0 padding if length of trajectory is less than max_length.
-
-        shapes:
-            expert["states"] =  [num_experts, max_length, state_space]
-            expert["actions"] =  [num_experts, max_length, action_space]
-            expert["rewards"] =  [num_experts, max_length]
-            expert["lengths"] =  [num_experts]
-    """
-
-    def __init__(self,
-                 expert_location: str,
-                 num_trajectories: int = 4,
-                 subsample_frequency: int = 20,
-                 seed: int = 0):
-        """Subsamples an expert dataset from saved expert trajectories.
-
-        Args:
-            expert_location:          Location of saved expert trajectories.
-            num_trajectories:         Number of expert trajectories to sample (randomized).
-            subsample_frequency:      Subsamples each trajectory at specified frequency of steps.
-            deterministic:            If true, sample determinstic expert trajectories.
-        """
-        all_trajectories = load_trajectories(expert_location, num_trajectories, seed)
-        self.trajectories = {}
-
-        # Randomize start index of each trajectory for subsampling
-        # start_idx = torch.randint(0, subsample_frequency, size=(num_trajectories,)).long()
-
-        # Subsample expert trajectories with every `subsample_frequency` step.
-        for k, v in all_trajectories.items():
-            data = v
-
-            if k != "lengths":
-                samples = []
-                for i in range(num_trajectories):
-                    samples.append(data[i][0::subsample_frequency])
-                self.trajectories[k] = samples
-            else:
-                # Adjust the length of trajectory after subsampling
-                self.trajectories[k] = np.array(data) // subsample_frequency
-
-        self.i2traj_idx = {}
-        self.length = self.trajectories["lengths"].sum().item()
-
-        del all_trajectories  # Not needed anymore
-        traj_idx = 0
-        i = 0
-
-        # Convert flattened index i to trajectory indx and offset within trajectory
-        self.get_idx = []
-
-        for _j in range(self.length):
-            while self.trajectories["lengths"][traj_idx].item() <= i:
-                i -= self.trajectories["lengths"][traj_idx].item()
-                traj_idx += 1
-
-            self.get_idx.append((traj_idx, i))
-            i += 1
-
-    def __len__(self) -> int:
-        """Return the length of the dataset."""
-        return self.length
-
-    def __getitem__(self, i):
-        traj_idx, i = self.get_idx[i]
-
-        states = self.trajectories["states"][traj_idx][i]
-        next_states = self.trajectories["next_states"][traj_idx][i]
-
-        # Rescale states and next_states to [0, 1] if are images
-        if isinstance(states, np.ndarray) and states.ndim == 3:
-            states = np.array(states) / 255.0
-        if isinstance(states, np.ndarray) and next_states.ndim == 3:
-            next_states = np.array(next_states) / 255.0
-
-        return (states,
-                next_states,
-                self.trajectories["actions"][traj_idx][i],
-                self.trajectories["rewards"][traj_idx][i],
-                self.trajectories["dones"][traj_idx][i])
-
-
-def load_trajectories(expert_location: str,
-                      num_trajectories: int = 10,
-                      seed: int = 0) -> Dict[str, Any]:
-    """Load expert trajectories
-
-    Args:
-        expert_location:          Location of saved expert trajectories.
-        num_trajectories:         Number of expert trajectories to sample (randomized).
-        deterministic:            If true, random behavior is switched off.
-
-    Returns:
-        Dict containing keys {"states", "lengths"} and optionally {"actions", "rewards"} with values
-        containing corresponding expert data attributes.
-    """
-    if os.path.isfile(expert_location):
-        # Load data from single file.
-        with open(expert_location, 'rb') as f:
-            trajs = read_file(expert_location, f)
-
-        rng = np.random.RandomState(seed)
-        # Sample random `num_trajectories` experts.
-        perm = np.arange(len(trajs["states"]))
-        perm = rng.permutation(perm)
-
-        idx = perm[:num_trajectories]
-        for k, v in trajs.items():
-            # if not torch.is_tensor(v):
-            #     v = np.array(v)  # convert to numpy array
-            trajs[k] = [v[i] for i in idx]
-
-    else:
-        raise ValueError(f"{expert_location} is not a valid path")
-    return trajs
-
-
-def read_file(path: str, file_handle: IO[Any]) -> Dict[str, Any]:
-    """Read file from the input path. Assumes the file stores dictionary data.
-
-    Args:
-        path:               Local or S3 file path.
-        file_handle:        File handle for file.
-
-    Returns:
-        The dictionary representation of the file.
-    """
-    if path.endswith("pt"):
-        data = torch.load(file_handle)
-    elif path.endswith("pkl"):
-        data = pickle.load(file_handle)
-    elif path.endswith("npy"):
-        data = np.load(file_handle, allow_pickle=True)
-        if data.ndim == 0:
-            data = data.item()
-    else:
-        raise NotImplementedError
-    return data
diff --git a/iq_learn/logger.py b/iq_learn/logger.py
deleted file mode 100644
index dcec353..0000000
--- a/iq_learn/logger.py
+++ /dev/null
@@ -1,228 +0,0 @@
-from torch.utils.tensorboard import SummaryWriter
-from collections import defaultdict
-import json
-import os
-import csv
-import shutil
-import torch
-import numpy as np
-from termcolor import colored
-
-COMMON_TRAIN_FORMAT = [
-    ('episode', 'E', 'int'),
-    ('step', 'S', 'int'),
-    ('episode_reward', 'R', 'float'),
-    ('duration', 'D', 'time')
-]
-
-COMMON_EVAL_FORMAT = [
-    ('episode', 'E', 'int'),
-    ('step', 'S', 'int'),
-    ('episode_reward', 'R', 'float')
-]
-
-
-AGENT_TRAIN_FORMAT = {
-    'sac': [
-        ('batch_reward', 'BR', 'float'),
-        ('actor_loss', 'ALOSS', 'float'),
-        ('critic_loss', 'CLOSS', 'float'),
-        ('alpha_loss', 'TLOSS', 'float'),
-        ('alpha_value', 'TVAL', 'float'),
-        ('actor_entropy', 'AENT', 'float')
-    ]
-}
-
-
-class AverageMeter(object):
-    def __init__(self):
-        self._sum = 0
-        self._count = 0
-
-    def update(self, value, n=1):
-        self._sum += value
-        self._count += n
-
-    def value(self):
-        return self._sum / max(1, self._count)
-
-
-class MetersGroup(object):
-    def __init__(self, file_name, formating):
-        self._csv_file_name = self._prepare_file(file_name, 'csv')
-        self._formating = formating
-        self._meters = defaultdict(AverageMeter)
-        self._csv_file = open(self._csv_file_name, 'w')
-        self._csv_writer = None
-
-    def _prepare_file(self, prefix, suffix):
-        file_name = f'{prefix}.{suffix}'
-        if os.path.exists(file_name):
-            os.remove(file_name)
-        return file_name
-
-    def log(self, key, value, n=1):
-        self._meters[key].update(value, n)
-
-    def _prime_meters(self):
-        data = dict()
-        for key, meter in self._meters.items():
-            if key.startswith('train'):
-                key = key[len('train') + 1:]
-            else:
-                key = key[len('eval') + 1:]
-            key = key.replace('/', '_')
-            data[key] = meter.value()
-        return data
-
-    def _dump_to_csv(self, data):
-        if self._csv_writer is None:
-            self._csv_writer = csv.DictWriter(self._csv_file,
-                                              fieldnames=sorted(data.keys()),
-                                              restval=0.0)
-            self._csv_writer.writeheader()
-        self._csv_writer.writerow(data)
-        self._csv_file.flush()
-
-    def _format(self, key, value, ty):
-        if ty == 'int':
-            value = int(value)
-            return f'{key}: {value}'
-        elif ty == 'float':
-            return f'{key}: {value:.04f}'
-        elif ty == 'time':
-            return f'{key}: {value:04.1f} s'
-        else:
-            raise f'invalid format type: {ty}'
-
-    def _dump_to_console(self, data, prefix):
-        prefix = colored(prefix, 'yellow' if prefix == 'train' else 'green')
-        pieces = [f'| {prefix: <14}']
-        for key, disp_key, ty in self._formating:
-            value = data.get(key, 0)
-            pieces.append(self._format(disp_key, value, ty))
-        print(' | '.join(pieces))
-
-    def dump(self, step, prefix, save=True):
-        if len(self._meters) == 0:
-            return
-        if save:
-            data = self._prime_meters()
-            data['step'] = step
-            self._dump_to_csv(data)
-            self._dump_to_console(data, prefix)
-        self._meters.clear()
-
-
-class Logger(object):
-    def __init__(self,
-                 log_dir,
-                 save_tb=False,
-                 log_frequency=10000,
-                 agent='sac'):
-        self._log_dir = log_dir
-        self._log_frequency = log_frequency
-        if save_tb:
-            tb_dir = os.path.join(log_dir, 'tb')
-            if os.path.exists(tb_dir):
-                try:
-                    shutil.rmtree(tb_dir)
-                except:
-                    print("logger.py warning: Unable to remove tb directory")
-                    pass
-            self._sw = SummaryWriter(tb_dir)
-        else:
-            self._sw = None
-        # each agent has specific output format for training
-        assert agent in AGENT_TRAIN_FORMAT
-        train_format = COMMON_TRAIN_FORMAT + AGENT_TRAIN_FORMAT[agent]
-        self._train_mg = MetersGroup(os.path.join(log_dir, 'train'),
-                                     formating=train_format)
-        self._eval_mg = MetersGroup(os.path.join(log_dir, 'eval'),
-                                    formating=COMMON_EVAL_FORMAT)
-
-    def _should_log(self, step, log_frequency):
-        log_frequency = log_frequency or self._log_frequency
-        return step % log_frequency == 0
-
-    def _try_sw_log(self, key, value, step):
-        if self._sw is not None:
-            self._sw.add_scalar(key, value, step)
-
-    def _try_sw_log_video(self, key, frames, step):
-        if self._sw is not None:
-            frames = torch.from_numpy(np.array(frames))
-            frames = frames.unsqueeze(0)
-            self._sw.add_video(key, frames, step, fps=30)
-
-    def _try_sw_log_histogram(self, key, histogram, step):
-        if self._sw is not None:
-            self._sw.add_histogram(key, histogram, step)
-
-    def log(self, key, value, step, n=1, log_frequency=1):
-        if not self._should_log(step, log_frequency):
-            return
-        assert key.startswith('train') or key.startswith('eval')
-        if type(value) == torch.Tensor:
-            value = value.item()
-        self._try_sw_log(key, value / n, step)
-        mg = self._train_mg if key.startswith('train') else self._eval_mg
-        mg.log(key, value, n)
-
-    def log_param(self, key, param, step, log_frequency=None):
-        if not self._should_log(step, log_frequency):
-            return
-        self.log_histogram(key + '_w', param.weight.data, step)
-        if hasattr(param.weight, 'grad') and param.weight.grad is not None:
-            self.log_histogram(key + '_w_g', param.weight.grad.data, step)
-        if hasattr(param, 'bias') and hasattr(param.bias, 'data'):
-            self.log_histogram(key + '_b', param.bias.data, step)
-            if hasattr(param.bias, 'grad') and param.bias.grad is not None:
-                self.log_histogram(key + '_b_g', param.bias.grad.data, step)
-
-    def log_video(self, key, frames, step, log_frequency=None):
-        if not self._should_log(step, log_frequency):
-            return
-        assert key.startswith('train') or key.startswith('eval')
-        self._try_sw_log_video(key, frames, step)
-
-    def log_histogram(self, key, histogram, step, log_frequency=None):
-        if not self._should_log(step, log_frequency):
-            return
-        assert key.startswith('train') or key.startswith('eval')
-        self._try_sw_log_histogram(key, histogram, step)
-
-    def dump(self, step, save=True, ty=None):
-        if ty is None:
-            self._train_mg.dump(step, 'train', save)
-            self._eval_mg.dump(step, 'eval', save)
-        elif ty == 'eval':
-            self._eval_mg.dump(step, 'eval', save)
-        elif ty == 'train':
-            self._train_mg.dump(step, 'train', save)
-        else:
-            raise f'invalid log type: {ty}'
-
-
-# def setup_logger(filepath):
-#     file_formatter = logging.Formatter(
-#         "[%(asctime)s %(filename)s:%(lineno)s] %(levelname)-8s %(message)s",
-#         datefmt='%Y-%m-%d %H:%M:%S',
-#     )
-#     logger = logging.getLogger('example')
-#     handler = logging.StreamHandler()
-#     handler.setFormatter(file_formatter)
-#     logger.addHandler(handler)
-
-#     file_handle_name = "file"
-#     if file_handle_name in [h.name for h in logger.handlers]:
-#         return
-#     if os.path.dirname(filepath) is not '':
-#         if not os.path.isdir(os.path.dirname(filepath)):
-#             os.makedirs(os.path.dirname(filepath))
-#     file_handle = logging.FileHandler(filename=filepath, mode="a")
-#     file_handle.set_name(file_handle_name)
-#     file_handle.setFormatter(file_formatter)
-#     logger.addHandler(file_handle)
-#     logger.setLevel(logging.DEBUG)
-#     return logger
diff --git a/iq_learn/memory.py b/iq_learn/memory.py
deleted file mode 100644
index c1fe83c..0000000
--- a/iq_learn/memory.py
+++ /dev/null
@@ -1,70 +0,0 @@
-from collections import deque
-import numpy as np
-import random
-import torch
-
-from wrappers.atari_wrapper import LazyFrames
-from expert.expert_dataset import ExpertDataset
-
-
-class Memory(object):
-    def __init__(self, memory_size: int, seed: int = 0) -> None:
-        random.seed(seed)
-        self.memory_size = memory_size
-        self.buffer = deque(maxlen=self.memory_size)
-
-    def add(self, experience) -> None:
-        self.buffer.append(experience)
-
-    def size(self):
-        return len(self.buffer)
-
-    def sample(self, batch_size: int, continuous: bool = True):
-        if batch_size > len(self.buffer):
-            batch_size = len(self.buffer)
-        if continuous:
-            rand = random.randint(0, len(self.buffer) - batch_size)
-            return [self.buffer[i] for i in range(rand, rand + batch_size)]
-        else:
-            indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)
-            return [self.buffer[i] for i in indexes]
-
-    def clear(self):
-        self.buffer.clear()
-
-    def save(self, path):
-        b = np.asarray(self.buffer)
-        print(b.shape)
-        np.save(path, b)
-
-    def load(self, path, num_trajs, sample_freq, seed):
-        # If path has no extension add npy
-        if not path.endswith("pkl"):
-            path += '.npy'
-        data = ExpertDataset(path, num_trajs, sample_freq, seed)
-        # data = np.load(path, allow_pickle=True)
-        for i in range(len(data)):
-            self.add(data[i])
-
-    def get_samples(self, batch_size, device):
-        batch = self.sample(batch_size, False)
-
-        batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(
-            *batch)
-
-        # Scale obs for atari
-        # TODO: Use flags
-        if isinstance(batch_state[0], LazyFrames):
-            batch_state = np.array(batch_state) / 255.0
-        if isinstance(batch_next_state[0], LazyFrames):
-            batch_next_state = np.array(batch_next_state) / 255.0
-
-        batch_state = torch.as_tensor(batch_state, dtype=torch.float, device=device)
-        batch_next_state = torch.as_tensor(batch_next_state, dtype=torch.float, device=device)
-        batch_action = torch.as_tensor(batch_action, dtype=torch.float, device=device)
-        if batch_action.ndim == 1:
-            batch_action = batch_action.unsqueeze(1)
-        batch_reward = torch.as_tensor(batch_reward, dtype=torch.float, device=device).unsqueeze(1)
-        batch_done = torch.as_tensor(batch_done, dtype=torch.float, device=device).unsqueeze(1)
-
-        return batch_state, batch_next_state, batch_action, batch_reward, batch_done
diff --git a/iq_learn/pointmaze/results/sac_implicit_PointMazeRight-v0_actor b/iq_learn/pointmaze/results/sac_implicit_PointMazeRight-v0_actor
deleted file mode 100644
index 2d03bde..0000000
Binary files a/iq_learn/pointmaze/results/sac_implicit_PointMazeRight-v0_actor and /dev/null differ
diff --git a/iq_learn/pointmaze/results/sac_implicit_PointMazeRight-v0_critic b/iq_learn/pointmaze/results/sac_implicit_PointMazeRight-v0_critic
deleted file mode 100644
index 0d8a607..0000000
Binary files a/iq_learn/pointmaze/results/sac_implicit_PointMazeRight-v0_critic and /dev/null differ
diff --git a/iq_learn/pointmaze/sac_implicit_PointMazeRight-v0_actor b/iq_learn/pointmaze/sac_implicit_PointMazeRight-v0_actor
deleted file mode 100644
index 673d732..0000000
Binary files a/iq_learn/pointmaze/sac_implicit_PointMazeRight-v0_actor and /dev/null differ
diff --git a/iq_learn/pointmaze/sac_implicit_PointMazeRight-v0_critic b/iq_learn/pointmaze/sac_implicit_PointMazeRight-v0_critic
deleted file mode 100644
index 8b4bf4b..0000000
Binary files a/iq_learn/pointmaze/sac_implicit_PointMazeRight-v0_critic and /dev/null differ
diff --git a/iq_learn/requirements.txt b/iq_learn/requirements.txt
deleted file mode 100644
index e0b1766..0000000
--- a/iq_learn/requirements.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-gym[box2d]==0.17.1
-hydra-core==1.0.6
-stable_baselines3==1.0
-tensorboard==2.4.0
-tensorboardX==2.1
-tensorflow==2.4.0
-torch==1.7.1
-torchvision==0.8.2
-tornado==5.1.1
-tqdm==4.42.1
-wandb
-opencv-python==4.5.1.48
-atari-py==0.2.6
-gym_minigrid==1.0.2
-mujoco_py==2.0.2
\ No newline at end of file
diff --git a/iq_learn/run_atari.sh b/iq_learn/run_atari.sh
deleted file mode 100644
index 6d7bdab..0000000
--- a/iq_learn/run_atari.sh
+++ /dev/null
@@ -1,12 +0,0 @@
-#!/usr/bin/env bash
-
-# Atari experiments (Default: Use 20 expert demo)
-
-# Pong
-python train_iq.py agent=softq env=pong agent.init_temperature=1e-3 method.loss=value_mix method.chi=True seed=0
-
-# Breakout
-python train_iq.py agent=softq env=breakout agent.init_temperature=1e-3 method.loss=value_mix method.chi=True seed=0
-
-# Space Invaders
-python train_iq.py agent=softq env=space agent.init_temperature=1e-3 method.loss=value_mix method.chi=True seed=0 
diff --git a/iq_learn/run_mujoco.sh b/iq_learn/run_mujoco.sh
deleted file mode 100644
index 6fdf1a0..0000000
--- a/iq_learn/run_mujoco.sh
+++ /dev/null
@@ -1,16 +0,0 @@
-#!/usr/bin/env bash
-
-# Train on Mujoco environements (Default: Use 10 expert demo)
-# Set eval.demos=1 for using one expert demo.
-
-# Hopper-v2
-python train_iq.py env=hopper agent=sac eval.demos=10 method.loss=v0 method.regularize=True agent.actor_lr=3e-5 seed=0
-
-# HalfCheetah-v2
-python train_iq.py env=cheetah agent=sac eval.demos=10 method.loss=value method.regularize=True agent.actor_lr=3e-05 seed=0
-
-# Ant-v2
-python train_iq.py env=ant agent=sac eval.demos=10 method.loss=value method.regularize=True agent.actor_lr=3e-05 agent.init_temperature=0.001 seed=0
-
-# Walker2d-v2
-python train_iq.py env=walker agent=sac eval.demos=10 method.loss=v0 method.regularize=True agent.actor_lr=3e-05 seed=0
\ No newline at end of file
diff --git a/iq_learn/run_offline.sh b/iq_learn/run_offline.sh
deleted file mode 100644
index ff371bc..0000000
--- a/iq_learn/run_offline.sh
+++ /dev/null
@@ -1,13 +0,0 @@
-#!/usr/bin/env bash
-
-# Offline Imitation learning experiments (Default: Use 10 expert demo)
-# Set eval.demos=1 for using one expert demo, reduce train.batch if the expert buffer is too small.
-
-# Acrobot-v1
-python train_iq.py agent=softq env=acrobot eval.demos=10 eval.subsample_freq=5 method.chi=True method.loss=value_expert seed=0
-
-# CartPole-v1
-python train_iq.py agent=softq env=cartpole eval.demos=10 eval.subsample_freq=20 method.chi=True method.loss=value_expert agent.init_temperature=0.001 seed=0
-
-# LunarLander-v2
-python train_iq.py agent=softq env=lunarlander eval.demos=10 eval.subsample_freq=5 method.chi=True method.loss=value_expert agent.init_temperature=0.001 seed=0
\ No newline at end of file
diff --git a/iq_learn/test_iq.py b/iq_learn/test_iq.py
index 32b6848..6d2ac30 100644
--- a/iq_learn/test_iq.py
+++ b/iq_learn/test_iq.py
@@ -5,15 +5,14 @@ import torch
 import numpy as np
 import os
 from omegaconf import DictConfig, OmegaConf
-
-from make_envs import make_env
-from agent import make_agent
-from utils import evaluate
 from scipy.stats import spearmanr, pearsonr
 import matplotlib.pyplot as plt
 import seaborn as sns
 import wandb
 
+from make_envs import make_env
+from agent import make_agent
+from utils.utils import evaluate
 
 def get_args(cfg: DictConfig):
     cfg.device = "cuda:0" if torch.cuda.is_available() else "cpu"
diff --git a/iq_learn/test_rl.py b/iq_learn/test_rl.py
index 8c3618d..8a4b5c6 100644
--- a/iq_learn/test_rl.py
+++ b/iq_learn/test_rl.py
@@ -7,7 +7,7 @@ from omegaconf import DictConfig, OmegaConf
 
 from make_envs import make_env
 from agent import make_agent
-from utils import eval_mode
+from utils.utils import eval_mode
 
 
 def get_args(cfg: DictConfig):
diff --git a/iq_learn/train_iq.py b/iq_learn/train_iq.py
index 561dc43..e7ce26e 100644
--- a/iq_learn/train_iq.py
+++ b/iq_learn/train_iq.py
@@ -9,19 +9,17 @@ import types
 import hydra
 import numpy as np
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
 import wandb
 from omegaconf import DictConfig, OmegaConf
 from tensorboardX import SummaryWriter
-from torch.autograd import Variable, grad
 
 from wrappers.atari_wrapper import LazyFrames
-from logger import Logger
 from make_envs import make_env
-from memory import Memory
+from dataset.memory import Memory
 from agent import make_agent
-from utils import eval_mode, get_concat_samples, evaluate, soft_update, hard_update
+from utils.utils import eval_mode, get_concat_samples, evaluate, soft_update, hard_update
+from utils.logger import Logger
 
 torch.set_num_threads(2)
 
@@ -44,22 +42,25 @@ def main(cfg: DictConfig):
     np.random.seed(args.seed)
     torch.manual_seed(args.seed)
 
-    env_args = args.env
+    device = torch.device(args.device)
+    if device.type == 'cuda' and torch.cuda.is_available() and args.cuda_deterministic:
+        torch.backends.cudnn.benchmark = False
+        torch.backends.cudnn.deterministic = True
 
+    env_args = args.env
     env = make_env(args)
     eval_env = make_env(args)
+
     # Seed envs
     env.seed(args.seed)
     eval_env.seed(args.seed + 10)
 
     REPLAY_MEMORY = int(env_args.replay_mem)
     INITIAL_MEMORY = int(env_args.initial_mem)
-    UPDATE_STEPS = int(env_args.update_steps)
     EPISODE_STEPS = int(env_args.eps_steps)
     EPISODE_WINDOW = int(env_args.eps_window)
     LEARN_STEPS = int(env_args.learn_steps)
-
-    INITIAL_STATES = 128
+    INITIAL_STATES = 128  # Num initial states to use to calculate value of initial state distribution s_0
 
     agent = make_agent(env, args)
 
@@ -71,25 +72,31 @@ def main(cfg: DictConfig):
         else:
             print("[Attention]: Do not find checkpoint {}".format(args.pretrain))
 
+    # Load expert data
     expert_memory_replay = Memory(REPLAY_MEMORY//2, args.seed)
     expert_memory_replay.load(hydra.utils.to_absolute_path(f'experts/{args.env.demo}'),
-                              num_trajs=args.eval.demos,
-                              sample_freq=args.eval.subsample_freq,
+                              num_trajs=args.expert.demos,
+                              sample_freq=args.expert.subsample_freq,
                               seed=args.seed + 42)
     print(f'--> Expert memory size: {expert_memory_replay.size()}')
 
     online_memory_replay = Memory(REPLAY_MEMORY//2, args.seed+1)
 
+    # Setup logging
     ts_str = datetime.datetime.fromtimestamp(time.time()).strftime("%Y-%m-%d_%H-%M-%S")
     log_dir = os.path.join(args.log_dir, args.env.name, args.exp_name, ts_str)
     writer = SummaryWriter(log_dir=log_dir)
     print(f'--> Saving logs at: {log_dir}')
     # TODO: Fix logging
-    logger = Logger(args.log_dir)
+    logger = Logger(args.log_dir,
+                    log_frequency=args.log_interval,
+                    writer=writer,
+                    save_tb=True,
+                    agent=args.agent.name)
 
     steps = 0
 
-    # track avg. reward and scores
+    # track mean reward and scores
     scores_window = deque(maxlen=EPISODE_WINDOW)  # last N scores
     rewards_window = deque(maxlen=EPISODE_WINDOW)  # last N rewards
     best_eval_returns = -np.inf
@@ -98,19 +105,23 @@ def main(cfg: DictConfig):
     begin_learn = False
     episode_reward = 0
 
+    # Sample initial states from env
     state_0 = [env.reset()] * INITIAL_STATES
     if isinstance(state_0[0], LazyFrames):
         state_0 = np.array(state_0) / 255.0
-    state_0 = torch.FloatTensor(state_0).to(args.device)
-    print(state_0.shape)
+    state_0 = torch.FloatTensor(np.array(state_0)).to(args.device)
 
     for epoch in count():
         state = env.reset()
         episode_reward = 0
         done = False
+
+        start_time = time.time()
         for episode_step in range(EPISODE_STEPS):
+
             if steps < args.num_seed_steps:
-                action = env.action_space.sample()  # Sample random action
+                # Seed replay buffer with random actions
+                action = env.action_space.sample()
             else:
                 with eval_mode(agent):
                     action = agent.choose_action(state, sample=True)
@@ -122,26 +133,28 @@ def main(cfg: DictConfig):
                 eval_returns, eval_timesteps = evaluate(agent, eval_env, num_episodes=args.eval.eps)
                 returns = np.mean(eval_returns)
                 learn_steps += 1  # To prevent repeated eval at timestep 0
-                writer.add_scalar('Rewards/eval_rewards', returns,
-                                  global_step=learn_steps)
-                print('EVAL\tEp {}\tAverage reward: {:.2f}\t'.format(epoch, returns))
-                writer.add_scalar(
-                    'Success/eval', np.mean((np.array(eval_returns) > 200)), global_step=epoch)
+                logger.log('eval/episode_reward', returns, learn_steps)
+                logger.log('eval/episode', epoch, learn_steps)
+                logger.dump(learn_steps)
+
+                # print('EVAL\tEp {}\tAverage reward: {:.2f}\t'.format(epoch, returns))
 
                 if returns > best_eval_returns:
+                    # Store best eval returns
                     best_eval_returns = returns
                     wandb.run.summary["best_returns"] = best_eval_returns
                     save(agent, epoch, args, output_dir='results_best')
 
-            # allow infinite bootstrap
+            # only store done true when episode finishes without hitting timelimit (allow infinite bootstrap)
             done_no_lim = done
             if str(env.__class__.__name__).find('TimeLimit') >= 0 and episode_step + 1 == env._max_episode_steps:
                 done_no_lim = 0
             online_memory_replay.add((state, next_state, action, reward, done_no_lim))
 
             if online_memory_replay.size() > INITIAL_MEMORY:
+                # Start learning
                 if begin_learn is False:
-                    print('learn begin!')
+                    print('Learn begins!')
                     begin_learn = True
 
                 learn_steps += 1
@@ -166,14 +179,13 @@ def main(cfg: DictConfig):
                 break
             state = next_state
 
-        writer.add_scalar('episodes', epoch, global_step=learn_steps)
-
         rewards_window.append(episode_reward)
-        scores_window.append(float(episode_reward > 200))
-        writer.add_scalar('Rewards/train_reward', np.mean(rewards_window), global_step=epoch)
-        writer.add_scalar('Success/train', np.mean(scores_window), global_step=epoch)
 
-        print('TRAIN\tEp {}\tAverage reward: {:.2f}\t'.format(epoch, np.mean(rewards_window)))
+        logger.log('train/episode', epoch, learn_steps)
+        logger.log('train/episode_reward', episode_reward, learn_steps)
+        logger.log('train/duration', time.time() - start_time, learn_steps)
+        logger.dump(learn_steps, save=begin_learn)
+        # print('TRAIN\tEp {}\tAverage reward: {:.2f}\t'.format(epoch, np.mean(rewards_window)))
         save(agent, epoch, args, output_dir='results')
 
 
@@ -374,6 +386,7 @@ def ilr_update_critic(self, policy_batch, expert_batch, logger, step):
         losses['regularize_loss'] = chi2_loss.item()
 
     losses['total_loss'] = loss.item()
+    logger.log('train_critic/loss', loss, step)
 
     # Optimize the critic
     self.critic_optimizer.zero_grad()
diff --git a/iq_learn/train_rl.py b/iq_learn/train_rl.py
index 9b63dd1..58ef47a 100644
--- a/iq_learn/train_rl.py
+++ b/iq_learn/train_rl.py
@@ -4,6 +4,8 @@ import random
 import time
 from collections import deque
 from itertools import count
+import types
+
 import hydra
 import numpy as np
 import torch
@@ -12,11 +14,11 @@ import wandb
 from omegaconf import DictConfig, OmegaConf
 from tensorboardX import SummaryWriter
 
-from logger import Logger
+from utils.logger import Logger
 from make_envs import make_env
-from memory import Memory
+from dataset.memory import Memory
 from agent import make_agent
-from utils import evaluate, eval_mode
+from utils.utils import evaluate, eval_mode
 
 torch.set_num_threads(2)
 
@@ -31,8 +33,8 @@ def get_args(cfg: DictConfig):
 @hydra.main(config_path="conf", config_name="config")
 def main(cfg: DictConfig):
     args = get_args(cfg)
-    wandb.init(project=args.env.name + '_rl', entity='iq-learn', config=args)
-    wandb.tensorboard.patch(save=False, pytorch=True)
+    wandb.init(project=args.env.name + '_rl', entity='iq-learn', 
+               sync_tensorboard=True, reinit=True, config=args)
 
     # set seeds
     random.seed(args.seed)
@@ -45,27 +47,23 @@ def main(cfg: DictConfig):
         torch.backends.cudnn.deterministic = True
 
     env_args = args.env
-
     env = make_env(args)
     eval_env = make_env(args)
     # Seed envs
     env.seed(args.seed)
-    eval_env.seed(args.seed + 1)
+    eval_env.seed(args.seed + 10)
 
     REPLAY_MEMORY = int(env_args.replay_mem)
     INITIAL_MEMORY = int(env_args.initial_mem)
-    UPDATE_STEPS = int(env_args.update_steps)
     EPISODE_STEPS = int(env_args.eps_steps)
     EPISODE_WINDOW = int(env_args.eps_window)
     LEARN_STEPS = int(env_args.learn_steps)
 
-    GAMMA = args.gamma
-    BATCH = args.train.batch
-
     agent = make_agent(env, args)
 
     memory_replay = Memory(REPLAY_MEMORY, args.seed)
 
+    # Setup logging
     ts_str = datetime.datetime.fromtimestamp(time.time()).strftime("%Y-%m-%d_%H-%M-%S")
     log_dir = os.path.join(args.log_dir, args.env.name, args.exp_name, ts_str)
     writer = SummaryWriter(log_dir=log_dir)
@@ -77,7 +75,7 @@ def main(cfg: DictConfig):
     learn_steps = 0
     begin_learn = False
 
-    # track avg. reward and scores
+    # track mean reward and scores
     rewards_window = deque(maxlen=EPISODE_WINDOW)  # last N rewards
     best_eval_returns = -np.inf
 
@@ -87,7 +85,8 @@ def main(cfg: DictConfig):
         done = False
         for episode_step in range(EPISODE_STEPS):
             if steps < args.num_seed_steps:
-                action = env.action_space.sample()  # Sample random action
+                # Seed replay buffer with random actions
+                action = env.action_space.sample()
             else:
                 with eval_mode(agent):
                     action = agent.choose_action(state, sample=True)
@@ -99,26 +98,31 @@ def main(cfg: DictConfig):
                 eval_returns, eval_timesteps = evaluate(agent, eval_env)
                 returns = np.mean(eval_returns)
                 learn_steps += 1  # To prevent repeated eval at timestep 0
-                writer.add_scalar('Rewards/eval_rewards', returns,
-                                  global_step=learn_steps)
+                writer.add_scalar('Rewards/eval_rewards', returns, global_step=learn_steps)
                 print('EVAL\tEp {}\tAverage reward: {:.2f}\t'.format(epoch, returns))
 
                 if returns > best_eval_returns:
+                    # Store best eval returns
                     best_eval_returns = returns
                     wandb.run.summary["best_returns"] = best_eval_returns
                     save(agent, epoch, args, output_dir='results_best')
 
-            # allow infinite bootstrap
+            # only store done true when episode finishes without hitting timelimit (allow infinite bootstrap)
             done_no_lim = done
             if str(env.__class__.__name__).find('TimeLimit') >= 0 and episode_step + 1 == env._max_episode_steps:
                 done_no_lim = 0
             memory_replay.add((state, next_state, action, reward, done_no_lim))
 
             if memory_replay.size() > INITIAL_MEMORY:
+                # Start learning
                 if begin_learn is False:
-                    print('learn begin!')
+                    print('Learn begins!')
                     begin_learn = True
                 learn_steps += 1
+                if learn_steps == LEARN_STEPS:
+                    print('Finished!')
+                    wandb.finish()
+                    return
 
                 losses = agent.update(memory_replay, logger, learn_steps)
 
@@ -128,10 +132,10 @@ def main(cfg: DictConfig):
 
             if done:
                 break
-
             state = next_state
 
         rewards_window.append(episode_reward)
+        writer.add_scalar('episodes', epoch, global_step=learn_steps)
         writer.add_scalar('Rewards/train_reward', np.mean(rewards_window), global_step=epoch)
         print('TRAIN\tEp {}\tAverage reward: {:.2f}\t'.format(epoch, np.mean(rewards_window)))
         save(agent, epoch, args, output_dir='results')
diff --git a/iq_learn/utils.py b/iq_learn/utils.py
deleted file mode 100644
index 379aa4b..0000000
--- a/iq_learn/utils.py
+++ /dev/null
@@ -1,137 +0,0 @@
-import numpy as np
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.autograd import Variable
-from torchvision.utils import make_grid, save_image
-
-
-class eval_mode(object):
-    def __init__(self, *models):
-        self.models = models
-
-    def __enter__(self):
-        self.prev_states = []
-        for model in self.models:
-            self.prev_states.append(model.training)
-            model.train(False)
-
-    def __exit__(self, *args):
-        for model, state in zip(self.models, self.prev_states):
-            model.train(state)
-        return False
-
-
-def evaluate(actor, env, num_episodes=10, vis=True):
-    """Evaluates the policy.
-    Args:
-      actor: A policy to evaluate.
-      env: Environment to evaluate the policy on.
-      num_episodes: A number of episodes to average the policy on.
-    Returns:
-      Averaged reward and a total number of steps.
-    """
-    total_timesteps = []
-    total_returns = []
-
-    while len(total_returns) < num_episodes:
-        state = env.reset()
-        done = False
-
-        with eval_mode(actor):
-            while not done:
-                action = actor.choose_action(state, sample=False)
-                next_state, reward, done, info = env.step(action)
-                state = next_state
-
-                if 'episode' in info.keys():
-                    total_returns.append(info['episode']['r'])
-                    total_timesteps.append(info['episode']['l'])
-
-    return total_returns, total_timesteps
-
-
-def weighted_softmax(x, weights):
-    x = x - torch.max(x, dim=0)[0]
-    return weights * torch.exp(x) / torch.sum(
-        weights * torch.exp(x), dim=0, keepdim=True)
-
-
-def soft_update(net, target_net, tau):
-    for param, target_param in zip(net.parameters(), target_net.parameters()):
-        target_param.data.copy_(tau * param.data +
-                                (1 - tau) * target_param.data)
-
-
-def hard_update(source, target):
-    for param, target_param in zip(source.parameters(), target.parameters()):
-        target_param.data.copy_(param.data)
-
-
-def weight_init(m):
-    """Custom weight init for Conv2D and Linear layers."""
-    if isinstance(m, nn.Linear):
-        nn.init.orthogonal_(m.weight.data)
-        if hasattr(m.bias, 'data'):
-            m.bias.data.fill_(0.0)
-
-
-class MLP(nn.Module):
-    def __init__(self,
-                 input_dim,
-                 hidden_dim,
-                 output_dim,
-                 hidden_depth,
-                 output_mod=None):
-        super().__init__()
-        self.trunk = mlp(input_dim, hidden_dim, output_dim, hidden_depth,
-                         output_mod)
-        self.apply(weight_init)
-
-    def forward(self, x):
-        return self.trunk(x)
-
-
-def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):
-    if hidden_depth == 0:
-        mods = [nn.Linear(input_dim, output_dim)]
-    else:
-        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]
-        for i in range(hidden_depth - 1):
-            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]
-        mods.append(nn.Linear(hidden_dim, output_dim))
-    if output_mod is not None:
-        mods.append(output_mod)
-    trunk = nn.Sequential(*mods)
-    return trunk
-
-
-def get_concat_samples(policy_batch, expert_batch, args):
-    online_batch_state, online_batch_next_state, online_batch_action, online_batch_reward, online_batch_done = policy_batch
-
-    expert_batch_state, expert_batch_next_state, expert_batch_action, expert_batch_reward, expert_batch_done = expert_batch
-
-    if not args.method.type == "sqil":
-        # convert expert reward to 0.
-        expert_batch_reward = torch.zeros_like(expert_batch_reward)
-
-    batch_state = torch.cat([online_batch_state, expert_batch_state], dim=0)
-    batch_next_state = torch.cat(
-        [online_batch_next_state, expert_batch_next_state], dim=0)
-    batch_action = torch.cat([online_batch_action, expert_batch_action], dim=0)
-    batch_reward = torch.cat([online_batch_reward, expert_batch_reward], dim=0)
-    batch_done = torch.cat([online_batch_done, expert_batch_done], dim=0)
-    is_expert = torch.cat([torch.zeros_like(online_batch_reward, dtype=torch.bool),
-                           torch.ones_like(expert_batch_reward, dtype=torch.bool)], dim=0)
-
-    return batch_state, batch_next_state, batch_action, batch_reward, batch_done, is_expert
-
-
-def save_state(tensor, path, num_states=5):
-    """Show stack framed of images consisting the state"""
-
-    tensor = tensor[:num_states]
-    B, C, H, W = tensor.shape
-    images = tensor.reshape(-1, 1, H, W).cpu()
-    save_image(images, path, nrow=num_states)
-    # make_grid(images)
diff --git a/iq_learn/vis/maze_vis.py b/iq_learn/vis/maze_vis.py
index 19e2dde..0b92cf2 100644
--- a/iq_learn/vis/maze_vis.py
+++ b/iq_learn/vis/maze_vis.py
@@ -1,6 +1,3 @@
-from utils import evaluate
-from agent import make_agent
-from make_envs import make_env
 from omegaconf import DictConfig, OmegaConf
 import torch
 import hydra
@@ -13,6 +10,11 @@ import pickle
 import matplotlib
 import os
 import wandb
+
+from utils.utils import evaluate
+from agent import make_agent
+from make_envs import make_env
+
 matplotlib.use('Agg')
 
 
